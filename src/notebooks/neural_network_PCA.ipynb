{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" neural network \"\"\"\n",
    "# importing packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from pickle import dump, load\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z          0.8255\n",
      "M_i      -23.1607\n",
      "u_band    21.0376\n",
      "g_band     20.621\n",
      "r_band    20.4777\n",
      "i_band    20.4211\n",
      "z_band     20.216\n",
      "ug        0.41668\n",
      "gr        0.14328\n",
      "ri        0.05654\n",
      "iz         0.2051\n",
      "zu        -0.8216\n",
      "Name: 0, dtype: object\n",
      "u_band    21.0376\n",
      "g_band     20.621\n",
      "r_band    20.4777\n",
      "i_band    20.4211\n",
      "z_band     20.216\n",
      "ug        0.41668\n",
      "gr        0.14328\n",
      "ri        0.05654\n",
      "iz         0.2051\n",
      "zu        -0.8216\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# visualize data\n",
    "df = pd.read_csv('../../data/dr14/TRAIN_dr14_err<.3.csv')\n",
    "print(df.iloc[0,[6,8,12,13,14,15,16,17,18,19,20,21]])# 12 features for AGN mass prediction (dr14)\n",
    "print(df.iloc[0,12:]) #10 features for AGN redshift predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original matched catalog had ~29k AGN.  Filtering out errors > .4 dex brings catalog to ~23,000 AGN.  I find that filtering out errors >.3 dex gives best performance.  This brings full catalog to ~20,300 AGN. In case of training for redshift, spec-z measurements are reliable so we can use the full catalog of ~29k AGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Black Hole dataset--default parameters are for training and predicting AGN mass.  Pass 'train=False' \n",
    "# for test-set and 'mass=False' for AGN redshift prediction.\n",
    "class BHDataset(Dataset):\n",
    "    def __init__(self, path, train=True, mass=True):\n",
    "        self.path = path\n",
    "        self.train = train\n",
    "        self.mass = mass\n",
    "        self.sc = StandardScaler()\n",
    "        self.pca = PCA(.95)\n",
    "        \n",
    "        if self.mass:\n",
    "            \n",
    "            if self.train:\n",
    "                self.data = pd.read_csv(self.path + 'TRAIN_dr14_err<.3.csv')\n",
    "                self.features = self.sc.fit_transform(np.asarray(self.data.iloc[:,[6,8,12,13,14,15,16,17,18,19,20,21]]))\n",
    "                self.pca.fit(self.features)\n",
    "                self.features = self.pca.transform(self.features)\n",
    "                dump(self.sc, open('train_scaler.pkl','wb'))\n",
    "                dump(self.pca, open('train_scaler_pca.pkl','wb'))\n",
    "        \n",
    "            else:\n",
    "                self.data = pd.read_csv(self.path + 'TEST_dr14_err<.3.csv')\n",
    "                self.sc = load(open('train_scaler.pkl','rb'))\n",
    "                self.pca = load(open('train_scaler_pca.pkl','rb'))\n",
    "                self.features = self.sc.transform(np.asarray(self.data.iloc[:,[6,8,12,13,14,15,16,17,18,19,20,21]]))\n",
    "                self.features = self.pca.transform(self.features)\n",
    "        else:\n",
    "            \n",
    "            if self.train:\n",
    "                self.data = pd.read_csv(self.path + 'TRAIN_dr14.csv')\n",
    "                self.features = self.sc.fit_transform(np.asarray(self.data.iloc[:,12:]))\n",
    "                self.pca.fit(self.features)\n",
    "                self.features = self.pca.transform(self.features)\n",
    "                dump(self.sc, open('train_scaler.pkl','wb'))\n",
    "                dump(self.pca, open('train_scaler_pca.pkl','wb'))\n",
    "        \n",
    "            else:\n",
    "                self.data = pd.read_csv(self.path + 'TEST_dr14.csv')\n",
    "                self.sc = load(open('train_scaler.pkl','rb'))\n",
    "                self.pca = load(open('train_scaler_pca.pkl','rb'))\n",
    "                self.features = self.sc.transform(np.asarray(self.data.iloc[:,12:]))\n",
    "                self.features=(self.pca.transform(self.features))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.mass:\n",
    "            \n",
    "            ID = torch.from_numpy(np.asarray(self.data.iloc[index,0]))\n",
    "            target = torch.from_numpy(np.asarray(self.data.iloc[index,5]))\n",
    "            features = torch.from_numpy(self.features[index].reshape(1,-1).squeeze())\n",
    "            return (ID, features, target)\n",
    "        \n",
    "        else:\n",
    "            ID = torch.from_numpy(np.asarray(self.data.iloc[index,0]))\n",
    "            target = torch.from_numpy(np.asarray(self.data.iloc[index,6]))\n",
    "            features = torch.from_numpy(self.features[index].reshape(1,-1).squeeze())\n",
    "            return (ID, features, target)\n",
    "\n",
    "        \n",
    "# define train and test datasets.  Train test split was done previously using sklearn.\n",
    "train_mass = BHDataset('../../data/dr14/')\n",
    "test_mass = BHDataset('../../data/dr14/', train=False)\n",
    "\n",
    "train_z = BHDataset('../../data/dr14/', mass=False)\n",
    "test_z = BHDataset('../../data/dr14/', mass=False, train=False)\n",
    "train_mass.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA reduces dimensionality of AGN BH Mass prediction from 12 features --> 6 features.  PCA reduces dimensionality of AGN redshift prediction from 10 features --> 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders with the datasets.  Only shuffle training sets.\n",
    "train_dl_mass = DataLoader(train_mass, batch_size=256, shuffle=True)\n",
    "test_dl_mass = DataLoader(test_mass, batch_size=256, shuffle=False)\n",
    "\n",
    "train_dl_z = DataLoader(train_z, batch_size=256, shuffle=True)\n",
    "test_dl_z = DataLoader(test_z, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default architecture is to predict AGN mass.  Pass 'mass=False' to predict redshift.\n",
    "class AGNet(nn.Module):\n",
    "    def __init__(self, mass=True):\n",
    "        super().__init__()\n",
    "        self.mass = mass\n",
    "        \n",
    "        if self.mass:\n",
    "\n",
    "            self.fc1 = nn.Linear(6, 32)\n",
    "            self.fc2 = nn.Linear(32, 64)\n",
    "            self.fc3 = nn.Linear(64, 128)\n",
    "            self.fc4 = nn.Linear(128, 128)\n",
    "            self.fc5 = nn.Linear(128, 64)\n",
    "            self.fc6 = nn.Linear(64, 32)\n",
    "            self.fc7 = nn.Linear(32, 1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.fc1 = nn.Linear(5, 32)\n",
    "            self.fc2 = nn.Linear(32, 64)\n",
    "            self.fc3 = nn.Linear(64, 128)\n",
    "            self.fc4 = nn.Linear(128, 128)\n",
    "            self.fc5 = nn.Linear(128, 64)\n",
    "            self.fc6 = nn.Linear(64, 32)\n",
    "            self.fc7 = nn.Linear(32, 1)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "# defining neural networks\n",
    "net_mass = AGNet()\n",
    "net_z = AGNet(mass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count parameters for reference.  For both redshift and mass, (training size/model parameters) < 1.\n",
    "(count_parameters(net_mass), count_parameters(net_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass does well with low learning rate (.001 or .005)\n",
    "# redshift does well with higher learning rate (.005 ok, .01 also good)\n",
    "# nn.SmoothL1Loss() for mass, nn.MSELoss() for redshift\n",
    "lr = .005\n",
    "optimizer = optim.AdamW(net_mass.parameters(), lr=lr)\n",
    "loss_function = nn.SmoothL1Loss() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop takes in epoch #, network, train loader, loss function, and optimizer.  \n",
    "def train(num_epochs, trainloader, testloader, mdl, batch_size=256):\n",
    "    \n",
    "    epoch_list = np.linspace(1, num_epochs , num = num_epochs)\n",
    "    train_loss_list, train_rmse_list, test_loss_list, test_rmse_list = list(), list(), list(), list()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "            train_pred, train_gt, test_pred, test_gt = list(), list(), list(), list()\n",
    "\n",
    "            for data in trainloader:\n",
    "\n",
    "                ID, features, ground_truth = data\n",
    "                train_gt.append(ground_truth.float())\n",
    "                output_train = mdl(features.float())\n",
    "                train_pred.append(output_train.float())\n",
    "                train_loss = loss_function(output_train.squeeze(), ground_truth.float().squeeze())\n",
    "\n",
    "                mdl.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            train_loss_list.append(train_loss.item()/batch_size)\n",
    "            train_ground_truth = torch.cat(train_gt).data\n",
    "            train_predictions = torch.cat(train_pred).data.flatten()\n",
    "            train_rmse = np.sqrt(metrics.mean_squared_error(train_ground_truth, train_predictions))\n",
    "            train_rmse_list.append(train_rmse)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for data_ in testloader:  \n",
    "\n",
    "                    ID_, features_, ground_truth_ = data_  \n",
    "                    test_gt.append(ground_truth_.float())\n",
    "                    output_test = mdl(features_.float()) \n",
    "                    test_pred.append(output_test.float())\n",
    "                    test_loss = loss_function(output_test.squeeze(), ground_truth_.squeeze()) # was.float() before\n",
    "                    \n",
    "                test_loss_list.append(test_loss.item()/batch_size)\n",
    "                test_ground_truth = torch.cat(test_gt).data.flatten()\n",
    "                test_predictions = torch.cat(test_pred).data.flatten()\n",
    "                test_rmse = np.sqrt(metrics.mean_squared_error(test_ground_truth, test_predictions))\n",
    "                test_rmse_list.append(test_rmse)\n",
    "                \n",
    "                print(f'EPOCH: {epoch+1}\\nTrain Loss: {train_loss.item():.5f} | Train RMSE: {train_rmse:.5f}\\nTest Loss: {test_loss.item():.5f} | Test RMSE: {test_rmse:.5f}')\n",
    "    return epoch_list, train_loss_list, test_loss_list, train_rmse_list, test_rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(epoch_list, train_loss, test_loss, train_rmse, test_rmse):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "#     ax1.set_xticks(len(epoch_list)/5)\n",
    "#     ax2.set_xticks(len(epoch_list)/5)\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(15)\n",
    "    \n",
    "    fig.suptitle('Loss and RMSE Curves', fontsize=16)\n",
    "    \n",
    "    ax1.set(xlabel = 'EPOCH', ylabel = 'LOSS')\n",
    "    ax1.plot(epoch_list, train_loss, label = 'Train Loss', color = 'blue')\n",
    "    ax1.plot(epoch_list, test_loss, label = 'Test Loss', color = 'orange', ls='--')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.set(xlabel = 'EPOCH', ylabel = 'RMSE')\n",
    "    ax2.set_ylim([0,1])\n",
    "    ax2.plot(epoch_list, train_rmse, label = 'Train RMSE', color = 'blue')\n",
    "    ax2.plot(epoch_list, test_rmse, label = 'Test RMSE', color = 'orange', ls='--') \n",
    "    ax2.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must change train function parameters depending on mass or z training\n",
    "epoch_list, train_loss, test_loss, train_rmse, test_rmse = train(1, train_dl_mass, test_dl_mass, net_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will return loss vs. epoch and RMSE vs. epoch curves\n",
    "plot_loss_curve(epoch_list, train_loss, test_loss, train_rmse, test_rmse)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current combination of number of epochs, loss function, and optimizer are what we found to work best.  We encourage users to explore other combinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(net_mass.state_dict(), '../../models/AGNet_dr14_6features_lr.005_50epoch.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loop outputs plot of results + dataframe of object ID, ground truth values, and network predictions\n",
    "def test(testloader, mdl, df=False):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_ID, test_pred, test_gt = list(), list(), list()\n",
    "\n",
    "        for data in testloader:\n",
    "            ID, features, ground_truth = data  \n",
    "            test_ID.append(ID.float())\n",
    "            test_gt.append(ground_truth.float())\n",
    "            output_test = mdl(features.float()) \n",
    "            test_pred.append(output_test.float())\n",
    "            test_loss = loss_function(output_test.squeeze(), ground_truth.squeeze())\n",
    "\n",
    "        test_ground_truth = torch.cat(test_gt).data\n",
    "        test_predictions = torch.cat(test_pred).data.flatten()\n",
    "        test_rmse = np.sqrt(metrics.mean_squared_error(test_ground_truth, test_predictions))\n",
    "        ID = torch.cat(test_ID).data\n",
    "        \n",
    "        plt.figure(figsize=[8,8])\n",
    "        plt.plot(test_ground_truth, test_ground_truth,color='black', label = 'Mass Ground Truth')\n",
    "        plt.scatter(test_ground_truth, test_predictions,s=2, color='blue', label = 'NN prediction')\n",
    "        plt.title('RMSE:' + str(test_rmse) + ' | R2: ' + str(metrics.r2_score(test_ground_truth,test_predictions)) + '| Pearson:' + str(pearsonr(test_ground_truth,test_predictions)[0]))\n",
    "        plt.xlabel('AGN Mass')\n",
    "        plt.ylabel('AGN Mass')\n",
    "#         plt.savefig('/Users/SnehPandya/Desktop/plot.png', facecolor='white')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    if df: \n",
    "        \n",
    "        results = pd.DataFrame({'my_ID':ID.numpy().astype('int'), 'ground_truth':test_ground_truth.numpy(), 'network_predictions':test_predictions.numpy() })\n",
    "        return results\n",
    "    \n",
    "    else:\n",
    "        print('pass df=True to create dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test(test_dl_mass, net_mass, df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting results with ground truth errors\n",
    "# font = {'family' : 'comic sans',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 12}\n",
    "\n",
    "# plt.rc('font', **font)\n",
    "# plt.figure(figsize=[10,10])\n",
    "# plt.scatter(matched['ground_truth'], matched['network_predictions'], s=2, color='blue',zorder=2, label = 'predictions')\n",
    "# plt.plot(matched['ground_truth'], matched['ground_truth'],color='black',zorder=1, label = 'ground truth')\n",
    "# plt.errorbar(matched['ground_truth'], matched['ground_truth'], yerr=matched['ERR'], alpha=.3,zorder=0,ecolor='orange',ls='', label = ' ground truth error')\n",
    "# plt.title('AGNet Mass Results')\n",
    "# plt.legend()\n",
    "# plt.xlim(6,11)\n",
    "# plt.ylim(6,11)\n",
    "# plt.savefig('/Users/SnehPandya/Desktop/error_plot.png',facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
